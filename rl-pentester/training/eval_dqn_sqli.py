import os
import sys
import numpy as np
from typing import Tuple

# --- Ensure project root is on sys.path ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from stable_baselines3 import DQN  # noqa: E402
from env.dvwa_sqli_env import DVWASQLiEnv  # noqa: E402


def run_episode(env: DVWASQLiEnv, model=None, deterministic: bool = True) -> Tuple[bool, int, float]:
    """
    Run one episode with either:
      - trained model (if model is not None)
      - random policy (if model is None)

    Returns: (success_flag, steps_taken, total_reward)
    """
    obs, info = env.reset()
    done = False
    truncated = False
    steps = 0
    total_reward = 0.0
    success = False

    while not (done or truncated):
        if model is None:
            action = env.action_space.sample()
        else:
            action, _ = model.predict(obs, deterministic=deterministic)

        obs, reward, done, truncated, step_info = env.step(int(action))
        total_reward += reward
        steps += 1

        if step_info.get("success", False):
            success = True

    return success, steps, total_reward


def main():
    env = DVWASQLiEnv(base_url="http://127.0.0.1:4280", max_steps=3)

    # Load trained model
    model_path = os.path.join("models", "dqn_dvwa_sqli_hard.zip")
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model not found at {model_path}. Train it first.")

    model = DQN.load(model_path)

    n_episodes = 50

    # Evaluate trained policy
    successes = []
    steps_list = []
    rewards_list = []

    print("[EVAL] Evaluating trained DQN agent...")
    for i in range(n_episodes):
        success, steps, total_reward = run_episode(env, model=model, deterministic=True)
        successes.append(success)
        steps_list.append(steps)
        rewards_list.append(total_reward)
        print(f"[AGENT] Episode {i}: success={success}, steps={steps}, reward={total_reward:.2f}")

    success_rate = np.mean(successes)
    avg_steps = np.mean(steps_list)
    avg_reward = np.mean(rewards_list)

    print("\n[EVAL] Trained agent results:")
    print(f"  Success rate: {success_rate*100:.1f}%")
    print(f"  Avg steps:    {avg_steps:.2f}")
    print(f"  Avg reward:   {avg_reward:.2f}")

    # Evaluate random policy for comparison
    rand_successes = []
    rand_steps_list = []
    rand_rewards_list = []

    print("\n[EVAL] Evaluating random policy...")
    for i in range(n_episodes):
        success, steps, total_reward = run_episode(env, model=None)
        rand_successes.append(success)
        rand_steps_list.append(steps)
        rand_rewards_list.append(total_reward)
        print(f"[RANDOM] Episode {i}: success={success}, steps={steps}, reward={total_reward:.2f}")

    rand_success_rate = np.mean(rand_successes)
    rand_avg_steps = np.mean(rand_steps_list)
    rand_avg_reward = np.mean(rand_rewards_list)

    print("\n[EVAL] Random policy results:")
    print(f"  Success rate: {rand_success_rate*100:.1f}%")
    print(f"  Avg steps:    {rand_avg_steps:.2f}")
    print(f"  Avg reward:   {rand_avg_reward:.2f}")

    env.close()


if __name__ == "__main__":
    main()
